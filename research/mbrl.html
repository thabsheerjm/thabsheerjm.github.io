<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Model Based RL - Manipulation tasks</title>
  
  <link rel="stylesheet" href="../css/styles.css">
  <link rel="stylesheet" href="../css/responsive.css">
  <link rel="stylesheet" href="../css/project.css">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>

<body>

  <a href="../index.html" class="home-link">Home</a>

  <div class="project-container">
    <h1>Learning Generalizable Robotic Manipulation</h1>

    <div class="project-links">
      <a href="#" class="project-link-button">
        <i class="fas fa-file-alt"></i> Report
      </a>
      <a href="#" class="project-link-button">
        <i class="fab fa-github"></i> Code
      </a>
    </div>
    <p class="project-date">October 2025</p>
    
    

    <h2>Introduction</h2>
    <p> 
      Current Robot learning methods are constrained by poor sample efficiency, limited generalization, and robot specific state and action representations. 
      Many successful methods rely on task-specific data collection, embodiment focused tuning, carefully engineered design spaces. 
      This project systematically studies robot learning methods along three fundamental axes: Sample efficiency, robustness across tasks and embodiments, and multimodal representations.     
    </p>

     <img src="../assets/thumbnails/expert_demo.gif" alt="Model Based RL animation" class="project-hero-image">
      <p class="image-caption">Demonstration of a learned policy performing a simple pick-and-place task.</p>

    <p>


    </p>

    <!-- <h2>Video Summary</h2>
    <p>a video.</p>
    
    <div class="video-wrapper">
      <iframe 
        src="https://www.youtube.com/embed/YOUR_VIDEO_ID" 
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
      </iframe>
    </div>  -->

    <h2>Axis I : Sample Efficiency</h2>
    <p>
      Sample efficient learning enables robots to learn complex real-world skills from fewer interactions with the environment. Collecting such data in the real-world is often time-consuming and expensive.
      Recent advances promise sample efficiency through methods such as imitation learning, Sim2Real transfer and active learning methods. However, many of these approahes require extensive task specific 
      data collection or careful fine-tuning to bridge the Sim2Real gap. In this work, we study different Robot learning appraoches entirely in simulation and compare their performance, aiming to identify 
      the key characteristics in generalizable manipulation. Sample efficiency is studied by analyzing how quickly each method learns, from task-specific to more general tasks, across different modalities.
      
      A single-object pick-and-place task was successfully demonstrated using the <a href="docs/pick_place.pdf" target="_blank">Soft-Actor Critic Algorithm</a>, 
      and we are currently exploring additional methods to present a comparative study.


    </p>
    
    <h3></h3>
    

    
    <h2>Axis II : Robustness</h2>
    <p>
      Maintaining consistent performance across diverse environments and tasks is imperative for deploying robots at scale. 
      This consistency ensures that systems remain safe, reliable, and predictable in dynamic real-world settings. 
      Our research explores the fundamental mechanisms of skill acquisition, the transfer of these skills to novel tasks and embodiments, 
      and the generalization of learned policies to handle unexpected environmental variations.

    </p>  

    <h2>Axis III : Multimodality</h2>
    <p>
      We also investigate diverse environment representations, including proprioception, visual feedback, and task-space coordinates, to evaluate the impact of 
      different sensory modalities on learning efficiency. By analyzing how these inputs interact, we aim to determine the optimal fusion of data required for high-precision control. 
      This exploration allows us to identify most robust signals for the agent to reliably execute its tasks. 
    </p>

  </div>
  
  </body>
</html>